{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00 Tensor是什么\n",
    "张量，描述任意物体复杂维度；    \n",
    "标量，零维表格；     \n",
    "向量，一维表格；    \n",
    "矩阵，二维表格；    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习；    \n",
    "$ y = wx + b $      \n",
    "w, b在未知时候，就是变量\n",
    "\n",
    "样本 模型       \n",
    "样本——Tensor；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor核心概念:     \n",
    "类型——创建；            \n",
    "属性——运算；\n",
    "操作——Numpy的互相转换；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的类型    \n",
    "float32, float16, uint8, int8, int6, int32, int64, bool     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img_Tensor的创建](img_Tensor的创建.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.FloatTensor\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.FloatTensor\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.FloatTensor\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n",
      "torch.FloatTensor\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "\n",
    "a = torch.Tensor(2, 3)\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "a = torch.ones(2, 3)\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "a = torch.eye(2, 3)\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.Tensor(2, 3)\n",
    "b = torch.zeros_like(b)\n",
    "b = torch.ones_like(b)\n",
    "\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2340, 0.9680],\n",
      "        [0.2138, 0.0225]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机的生成一些满足一定分布的张量；      \n",
    "正态分布，normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2340, 0.9680],\n",
      "        [0.2138, 0.0225]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "torch.normal(mean=0.0, std=torch.rand(5))\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2340, 0.9680],\n",
      "        [0.2138, 0.0225]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "torch.normal(mean=torch.rand(5), std=torch.rand(5))\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机生成均匀分布的数值uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8303,  0.6408],\n",
      "        [-0.9712, -0.6875]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(2, 2).uniform_(-1, 1)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成一些序列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 10, 1)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成等间隔的n个数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  6., 10.])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(2, 10, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 8, 9, 2, 3, 6, 0, 5, 1])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.randperm(10)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比numpy 和tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Tensor的属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个Tensor有torch.dtype, torch.device, torch.layout三种属性           \n",
    "+ torch.device标识了torch.Tensor对象在创建之后所存储在的设备的名称        \n",
    "+ torch.layout表示torch.Tensor内存布局的对象(稠密张量或者稀疏张量，稠密张量一般直接存在内存中的连续区域，稀疏张量只存储其非零坐标)        \n",
    "+ torch.tensor([1,2,3], dtype=torch.float32, device = torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稀疏张量的定义方式          \n",
    "torch.sparse_coo_tensor\n",
    "coo类型表示了非零元素的坐标形式     \n",
    "x = torch.sparse_coo_tensor(i, v, [2, 4])       \n",
    "稀疏与低秩      \n",
    "+ 稀疏描述了一个张量中的非零元素的多少        \n",
    "+ 秩表示了当前张量中的线性可表示关系      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dev = torch.device('cpu')\n",
    "dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2, 2], \n",
    "                 dtype=torch.float32,\n",
    "                 device=dev)\n",
    "print(a)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 3., 0.],\n",
      "        [0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor([[0, 1, 2], [0, 1, 2]])\n",
    "v = torch.tensor([1, 2, 3])\n",
    "a = torch.sparse_coo_tensor(i, v, (4, 4),\n",
    "                                   dtype=torch.float32,\n",
    "                                      device=dev).to_dense()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Tensor的算术运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法运算        \n",
    "c = a + b   \n",
    "c = torch.add(a, b)     \n",
    "a.add(b)        \n",
    "a.add_(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9102, 0.8461, 0.9555],\n",
      "        [0.7291, 0.0159, 0.0827]])\n",
      "tensor([[0.7974, 0.7807, 0.9750],\n",
      "        [0.2606, 0.5709, 0.2451]])\n",
      "tensor([[1.7076, 1.6269, 1.9305],\n",
      "        [0.9897, 0.5869, 0.3278]])\n",
      "tensor([[1.7076, 1.6269, 1.9305],\n",
      "        [0.9897, 0.5869, 0.3278]])\n",
      "tensor([[1.7076, 1.6269, 1.9305],\n",
      "        [0.9897, 0.5869, 0.3278]])\n",
      "tensor([[0.9102, 0.8461, 0.9555],\n",
      "        [0.7291, 0.0159, 0.0827]])\n",
      "tensor([[1.7076, 1.6269, 1.9305],\n",
      "        [0.9897, 0.5869, 0.3278]])\n",
      "tensor([[1.7076, 1.6269, 1.9305],\n",
      "        [0.9897, 0.5869, 0.3278]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(a + b)\n",
    "print(a.add(b))\n",
    "print(torch.add(a, b))\n",
    "print(a)\n",
    "print(a.add_(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减法运算        \n",
    "c = a - b       \n",
    "c = torch.sub(a, b)     \n",
    "a.sub(b)        \n",
    "a.sub_(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[0.9102, 0.8461, 0.9555],\n",
      "        [0.7291, 0.0159, 0.0827]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "print(a.sub(b))\n",
    "print(torch.sub(a, b))\n",
    "print(a)\n",
    "print(a.sub_(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乘法运算        \n",
    "+ 哈达玛积（element wise, 对应元素相乘）！注意与矩阵运算不同！      \n",
    "\n",
    "c = a * b   \n",
    "c = torch.mul(a, b)     \n",
    "a.mul(b)        \n",
    "a.mul_(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0899,  0.0511, -0.0191],\n",
      "        [ 0.1221, -0.3168, -0.0398]])\n",
      "tensor([[ 0.0899,  0.0511, -0.0191],\n",
      "        [ 0.1221, -0.3168, -0.0398]])\n",
      "tensor([[ 0.0899,  0.0511, -0.0191],\n",
      "        [ 0.1221, -0.3168, -0.0398]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[ 0.0899,  0.0511, -0.0191],\n",
      "        [ 0.1221, -0.3168, -0.0398]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "print(torch.mul(a, b))\n",
    "print(a.mul(b))\n",
    "print(a)\n",
    "print(a.mul_(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除法运算        \n",
    "c = a/b     \n",
    "c = torch.div(a, b)     \n",
    "a.div(b)        \n",
    "a.div_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n",
      "tensor([[ 0.0899,  0.0511, -0.0191],\n",
      "        [ 0.1221, -0.3168, -0.0398]])\n",
      "tensor([[ 0.1127,  0.0654, -0.0195],\n",
      "        [ 0.4685, -0.5550, -0.1624]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "print(torch.div(a, b))\n",
    "print(a.div(b))\n",
    "print(a)\n",
    "print(a.div_(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵运算        \n",
    "+ 二维矩阵乘法运算操作包括torch.mm()、torch.matmul()、@     \n",
    "\n",
    "    a = torch.ones(2, 1)        \n",
    "    b = torch.ones(1, 2)        \n",
    "    print(torch.mm(a, b))       \n",
    "    print(torch.matmul(a, b))       \n",
    "    print(a @ b)        \n",
    "    print(a.matmul(b))      \n",
    "    print(a.mm(b))      \n",
    "\n",
    "\n",
    "    a x b的矩阵必须要保证a是m乘n，b是n乘P   \n",
    "\n",
    "+ 对于高维的Tensor(dim > 2)，定义其矩阵乘法仅在最后的两个维度上，       \n",
    "要求前面的维度必须保持一致，就像矩阵的索引一样并且运算操只有torch.matmul()\n",
    "\n",
    "    a = torch.ones(1, 2, 3, 4)      \n",
    "    b = torch.ones(1, 2, 4, 4)      \n",
    "    print(a.matmul(b))      \n",
    "    print(torch.matmul(a, b))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[1., 1.]])\n",
      "_______________\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 1)        \n",
    "b = torch.ones(1, 2)        \n",
    "print(a)\n",
    "print(b)\n",
    "print(\"_______________\")\n",
    "print(a @ b)\n",
    "print(a.matmul(b))\n",
    "print(torch.matmul(a, b))\n",
    "print(torch.mm(a, b))\n",
    "print(a.mm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]],\n",
      "\n",
      "         [[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]]]])\n",
      "**************************\n",
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1, 2, 3, 4)\n",
    "b = torch.ones(1, 2, 4, 3)\n",
    "\n",
    "\n",
    "print(a.matmul(b))\n",
    "print(\"**************************\")\n",
    "print(a.matmul(b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幂运算      \n",
    "\n",
    "print(torch.pow(a, 2))      \n",
    "print(a.pow(2))     \n",
    "print(a**2)     \n",
    "print(a.pow_(2))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 8])\n",
      "tensor([1, 8])\n",
      "tensor([1, 8])\n",
      "tensor([1, 8])\n",
      "tensor([1, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "print(torch.pow(a, 3))\n",
    "print(a.pow(3))\n",
    "print(a**3)\n",
    "print(a.pow_(3))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e的n次方\n",
    "\n",
    "print(torch.exp(a))     \n",
    "b = a.exp_()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183, 7.3891])\n",
      "tensor([2.7183, 7.3891])\n",
      "tensor([  15.1543, 1618.1782])\n",
      "tensor([  15.1543, 1618.1782])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0])\n",
    "print(torch.exp(a))\n",
    "print(torch.exp_(a))\n",
    "print(a.exp())\n",
    "print(a.exp_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开方运算        \n",
    "\n",
    "print(a.sqrt())     \n",
    "print(a.sqrt_())        \n",
    "print(a)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  2.])\n",
      "tensor([3.1623, 1.4142])\n",
      "tensor([3.1623, 1.4142])\n",
      "tensor([3.1623, 1.4142])\n",
      "tensor([1.7783, 1.1892])\n",
      "tensor([1.7783, 1.1892])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10.0, 2.0])\n",
    "print(a)\n",
    "print(torch.sqrt(a))\n",
    "print(torch.sqrt_(a))\n",
    "print(a)\n",
    "print(a.sqrt())\n",
    "print(a.sqrt_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数运算        \n",
    "\n",
    "print(torch.log2(a))        \n",
    "print(torch.log10(a))       \n",
    "print(torch.log(a))     \n",
    "print(torch.log_(a))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3026, 0.6931])\n",
      "tensor([2.3026, 0.6931])\n",
      "tensor([ 0.8340, -0.3665])\n",
      "tensor([ 0.8340, -0.3665])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10.0, 2.0])\n",
    "print(torch.log(a))\n",
    "print(torch.log_(a))\n",
    "print(a.log())\n",
    "print(a.log_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Pytorch中的in-place操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ “就地”操作，即不允许使用临时变量。          \n",
    "+ 也称为原位操作。        \n",
    "+ x = x + y       \n",
    "+ add_，sub_, mul_等等        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 广播机制\n",
    "+ 广播机制：张量参数可以自动扩展为相同大小（类似于Numpy)\n",
    "+ 广播机制需要满足两个条件：\n",
    "    - 每个张量至少有一个维度\n",
    "    - 满足右对齐\n",
    "    - torch.rand(2, 1, 1)+torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7817, 0.9514, 0.8736],\n",
      "        [0.9672, 0.0618, 0.7672]])\n",
      "tensor([0.8277, 0.1685, 0.9580])\n",
      "tensor([[1.6094, 1.1199, 1.8316],\n",
      "        [1.7949, 0.2303, 1.7253]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05 取整和取余运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.8396, 7.6450],\n",
      "        [2.3176, 8.1592]])\n",
      "tensor([[6., 7.],\n",
      "        [2., 8.]])\n",
      "tensor([[7., 8.],\n",
      "        [3., 9.]])\n",
      "tensor([[7., 8.],\n",
      "        [2., 8.]])\n",
      "tensor([[6., 7.],\n",
      "        [2., 8.]])\n",
      "tensor([[0.8396, 0.6450],\n",
      "        [0.3176, 0.1592]])\n",
      "tensor([[0.8396, 1.6450],\n",
      "        [0.3176, 0.1592]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "a = a * 10\n",
    "print(a)\n",
    "\n",
    "print(torch.floor(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.round(a))\n",
    "print(torch.trunc(a))\n",
    "print(torch.frac(a))\n",
    "print(a % 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 比较运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.eq(input, other, out=None) # 按成员进行等式操作，相同返回True     \n",
    "torch.equal(tensor1, tensor2) # 判断两个张量是否有相同的size 和 elements，则为True      \n",
    "torch.ge(input, other, out=None) # 按成员进行大于等于操作，大于等于返回True     \n",
    "torch.gt(input, other, out=None) # 按成员进行大于操作，大于返回True     \n",
    "torch.le(input, other, out=None) # 按成员进行小于等于操作，小于等于返回True     \n",
    "torch.lt(input, other, out=None) # 按成员进行小于操作，小于返回True     \n",
    "torch.ne(input, other, out=None) # 按成员进行不等式操作，不相同返回True     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9842, 0.8974, 0.9463],\n",
      "        [0.8997, 0.9821, 0.0788]])\n",
      "tensor([[0.9022, 0.4454, 0.6583],\n",
      "        [0.7642, 0.7959, 0.1215]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "False\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True,  True, False]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True,  True, False]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False,  True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(torch.eq(a, b))\n",
    "print(torch.equal(a, b))\n",
    "print(torch.ge(a, b))\n",
    "print(torch.gt(a, b))\n",
    "print(torch.le(a, b))\n",
    "print(torch.lt(a, b))\n",
    "print(torch.ne(a, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 取前k大/前k小/第k小的数值及其索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ torch.sort(input, dim=None, descending = False, out=None) #对目标张量input沿着指定维按升序排序      \n",
    "+ torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) #沿着指定维度返回最大k个数值及其索引值      \n",
    "    这个一般用在深度学习中对loss函数进行定义时候            \n",
    "+ torch.kthvalue(input, k, dim=None, keepdim=False, out=None) #沿着指定维度返回第k个最小值及其索引值      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([1, 3, 4, 4, 5]),\n",
      "indices=tensor([0, 3, 1, 2, 4]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 4, 4, 3, 5])\n",
    "print(torch.sort(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.sort(\n",
      "values=tensor([[5, 4, 4, 3, 1],\n",
      "        [5, 3, 3, 2, 1]]),\n",
      "indices=tensor([[4, 1, 2, 3, 0],\n",
      "        [4, 1, 3, 0, 2]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 4, 4, 3, 5],\n",
    "                  [2, 3, 1, 3, 5]])\n",
    "print(a.shape)\n",
    "print(torch.sort(a, dim = 1, descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[2, 4, 5, 1, 5],\n",
      "        [2, 3, 3, 1, 4]]),\n",
      "indices=tensor([[0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 1]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[2, 4, 5, 1, 5]]),\n",
      "indices=tensor([[0, 0, 1, 0, 0]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[5],\n",
      "        [5]]),\n",
      "indices=tensor([[4],\n",
      "        [2]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[5, 4],\n",
      "        [5, 4]]),\n",
      "indices=tensor([[4, 1],\n",
      "        [2, 4]]))\n"
     ]
    }
   ],
   "source": [
    "##topk\n",
    "a = torch.tensor([[2, 4, 3, 1,5],\n",
    "                  [2, 3, 5, 1, 4]])\n",
    "print(a.shape)\n",
    "\n",
    "print(torch.topk(a, k = 2, dim=0))\n",
    "print(torch.topk(a, k = 1, dim=0))\n",
    "print(torch.topk(a, k = 1, dim=1))\n",
    "print(torch.topk(a, k = 2, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.kthvalue(\n",
      "values=tensor([2, 4, 5, 1, 5]),\n",
      "indices=tensor([1, 0, 1, 1, 0]))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([2, 2]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.kthvalue(a, k = 2, dim=0))\n",
    "print(torch.kthvalue(a, k = 2, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 判断是否为finite/inf/nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ torch.isfinite(tensor)/torch.isinf(tensor)/torch.isnan(tensor)      \n",
    "+ 返回一个标记元素是否为finite/inf/nan的mask张量      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9711, 0.0940, 0.8417],\n",
      "        [0.2347, 0.8983, 0.8136]])\n",
      "tensor([[inf, inf, inf],\n",
      "        [inf, inf, inf]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(a/0)\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isfinite(a/0))\n",
    "print(torch.isinf(a/0))\n",
    "print(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True])\n",
      "tensor([False, False, False])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([1, 2, np.nan])\n",
    "print(torch.isnan(a))\n",
    "print(torch.isinf(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 三角函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.abs(input, out=None)      \n",
    "torch.acos(input, out=None)     \n",
    "torch.asin(input, out=None)     \n",
    "torch.atan(input, out=None)     \n",
    "torch.atan2(input1, input2, out=None)       \n",
    "torch.cos(input, out=None)      \n",
    "torch.cosh(input, out=None)     \n",
    "torch.sin(input, out=None)      \n",
    "torch.sinh(input, out=None)     \n",
    "torch.tan(input, out=None)      \n",
    "torch.tanh(input, out=None)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1356, 0.2106, 0.3665],\n",
      "        [0.1437, 0.0881, 0.8351]])\n",
      "tensor([[0.9908, 0.9779, 0.9336],\n",
      "        [0.9897, 0.9961, 0.6711]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "b = torch.cos(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 其它的数学函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.asb()     \n",
    "torch.erf()     \n",
    "torch.erfinv()      \n",
    "torch.sigmoid()     \n",
    "torch.neg()     \n",
    "torch.reciprocal()      \n",
    "torch.rsqrt()       \n",
    "torch.sign()        \n",
    "torch.lerp()        \n",
    "torch.addcdiv()     \n",
    "torch.addcmul()     \n",
    "torch.cumprod()     \n",
    "torch.cumsum()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 统计学有关的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.mean()   #平均值      \n",
    "torch.sum()    #求和            \n",
    "torch.prod()   #求积            \n",
    "torch.max()    #最大值      \n",
    "torch.min()    #最小值      \n",
    "torch.argmax() #最大值索引      \n",
    "torch.argmin() #最小值索引      \n",
    "\n",
    "\n",
    "torch.median() #中位数      \n",
    "torch.std()    #标准差      \n",
    "torch.var()    #方差        \n",
    "torch.mode()   #众数        \n",
    "torch.histc()  #直方图统计      \n",
    "torch.bincount() #计数      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7180, 0.2160],\n",
      "        [0.5633, 0.5754]])\n",
      "tensor(0.5182)\n",
      "tensor(2.0727)\n",
      "tensor(0.0503)\n",
      "tensor(0.2134)\n",
      "tensor(0.0455)\n",
      "tensor(0.5633)\n",
      "torch.return_types.mode(\n",
      "values=tensor([0.2160, 0.5633]),\n",
      "indices=tensor([1, 0]))\n",
      "tensor(1.1003)\n",
      "tensor(0.)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "\n",
    "print(a)\n",
    "print(torch.mean(a))\n",
    "print(torch.sum(a))\n",
    "print(torch.prod(a))\n",
    "print(torch.std(a))\n",
    "print(torch.var(a))\n",
    "print(torch.median(a))\n",
    "print(torch.mode(a))\n",
    "print(torch.norm(a))\n",
    "print(torch.dist(a, a))\n",
    "print(torch.histc(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1165, 4.2924],\n",
      "        [2.8380, 5.6892]])\n",
      "tensor([0., 3., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "\n",
    "print(torch.histc(a, bins=4, min=0, max=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 分布函数torch.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如机器学习中的训练集和测试集的分布是否一致      \n",
    "\n",
    "- distributions包含可参数化的概率分布和采样函数             \n",
    "    - 得分函数           \n",
    "         - 强化学习中策略梯度方法的基础               \n",
    "    - pathwise derivative估计器       \n",
    "        - 变分自动编码器中的重新参数化技巧        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 随机抽样函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义随机种子          \n",
    "    - torch.manual_seed(seed)       \n",
    "- 定义随机满足的分布        \n",
    "    - torch.normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9720, 1.1360]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mean = torch.rand(1, 2)\n",
    "std  = torch.rand(1, 2)\n",
    "\n",
    "print(torch.normal(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 范数\n",
    "    - 在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即1.非负性；2齐次性；3.三角不等式\n",
    "    - 常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小\n",
    "\n",
    "- 0范数/1范数/2范数/p范数/核范数\n",
    "    - torch.dist(input, other, p=2)计算p范数\n",
    "    - torch.norm() 计算2范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补：向量与矩阵的范数\n",
    "\n",
    "范数（Norm）是一个数学概念，表示向量或矩阵的“大小”或“长度”，        \n",
    "它是衡量向量或矩阵在空间中某种距离的方式。在数学、物理学和计算机科学中，        \n",
    "范数常用于优化、距离计算、机器学习和信号处理等领域。\n",
    "\n",
    "以下是几种常见的范数：0范数、1范数、2范数、p范数、核范数，      \n",
    "它们通常用于向量和矩阵的计算中，不同的范数具有不同的特性和应用。        \n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 0 范数（零范数，L0 Norm）       \n",
    "**定义**：零范数并不是真正的范数，但它常常被用来计算一个向量或矩阵中非零元素的个数。        \n",
    "用公式表示为：      \n",
    "$ \\|x\\|_0 = \\text{非零元素的个数} $\n",
    "\n",
    "**性质**：\n",
    "- 零范数不是严格意义上的范数，因为它不满足**齐次性**（例如，$ \\| \\alpha x \\|_0 \\neq \\alpha \\| x \\|_0 $，对于标量 $ \\alpha $）。\n",
    "- 但是，零范数通常用于**稀疏性**相关的任务中，比如 **LASSO回归**、**特征选择**等。\n",
    "\n",
    "**应用**：\n",
    "- **稀疏表示**：通过零范数可以衡量一个向量或矩阵有多少个非零元素。      \n",
    "在机器学习中，通常希望得到一个具有较少非零元素的模型，这样的模型更加稀疏、简单、易于解释。      \n",
    "- **特征选择**：通过零范数来选出最重要的特征，抑制不重要的特征。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 1 范数（L1 Norm）\n",
    "**定义**：1范数是向量中所有元素绝对值的和，公式如下：\n",
    "$ \\|x\\|_1 = \\sum_i |x_i| $\n",
    "\n",
    "**性质**：\n",
    "- L1 范数会使得一些元素变为 0，因此常用于实现**稀疏化**。\n",
    "- 它对于噪声较敏感，但能够很好地处理**稀疏问题**。\n",
    "\n",
    "**应用**：\n",
    "- **LASSO回归**：L1范数用于正则化（惩罚模型的复杂度），     \n",
    "通过约束模型参数的 L1 范数来使得一些系数变为 0，从而选择重要特征。      \n",
    "- **稀疏编码和压缩感知**：L1范数用于找到稀疏表示，减少数据的冗余。      \n",
    "  \n",
    "---\n",
    "\n",
    "#### 3. 2 范数（L2 Norm）\n",
    "**定义**：2范数，也称为 **欧几里得范数**，是向量各元素的平方和的平方根，公式如下：\n",
    "$ \\|x\\|_2 = \\sqrt{\\sum_i x_i^2} $\n",
    "\n",
    "**性质**：\n",
    "- L2 范数与欧几里得距离相关，常用于计算两点之间的距离。\n",
    "- L2 范数的优点是其具有**平滑性**，不会导致像 L1 范数那样产生稀疏解。\n",
    "- L2 范数对于异常值更为**鲁棒**，相比 L1 范数，它的计算结果更为平滑。\n",
    "\n",
    "**应用**：\n",
    "- **Ridge回归**：L2范数用于正则化，通过约束参数的L2范数来控制模型的复杂度，避免过拟合。\n",
    "- **梯度下降算法**：L2范数常用于优化问题中，特别是当我们需要最小化误差平方和时。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. p 范数（Lp Norm）\n",
    "**定义**：p范数是对任意 $p \\geq 1$ 的扩展，公式如下：\n",
    "$ \\|x\\|_p = \\left( \\sum_i |x_i|^p \\right)^{1/p} $       \n",
    "当 $p = 1$ 时，得到的是 **L1范数**，当 $p = 2$ 时，得到的是 **L2范数**，        \n",
    "当 $p \\to \\infty$ 时，得到的是 **最大值范数**（即最大元素的绝对值）。       \n",
    "\n",
    "**性质**：\n",
    "- 当 $p = 1$，是 L1 范数；\n",
    "- 当 $p = 2$，是 L2 范数；\n",
    "- 随着 $p$ 增加，p 范数变得对大元素更敏感，尤其是 $p$ 较大时，p 范数更加注重向量中最大元素的影响。\n",
    "\n",
    "**应用**：\n",
    "- **正则化**：p范数的正则化（例如，L1正则化和L2正则化）用于约束模型的参数，防止过拟合。\n",
    "- **最优解的选择**：在机器学习中，根据问题需要选择不同的 p 范数进行优化，p 范数适用于不同的平滑性需求。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 核范数（Nuclear Norm）\n",
    "**定义**：核范数是一个矩阵的所有奇异值的和，也可以理解为矩阵的 **L1 范数** 对奇异值的扩展。     \n",
    "对于矩阵 $X$，其核范数定义为：      \n",
    "$ \\|X\\|_* = \\sum_i \\sigma_i $       \n",
    "其中 $ \\sigma_i $ 是矩阵 $X$ 的奇异值。     \n",
    "\n",
    "**性质**：\n",
    "- 核范数广泛用于矩阵补全问题中，能够得到**低秩矩阵**。\n",
    "- 核范数与 **L1 范数** 类似，但它作用于矩阵的奇异值，而不是矩阵元素本身。\n",
    "  \n",
    "**应用**：\n",
    "- **矩阵补全**：在缺失数据的情况下，使用核范数来逼近低秩矩阵，常用于推荐系统（如Netflix的电影推荐）。\n",
    "- **低秩矩阵恢复**：核范数正则化能够找到矩阵的低秩近似，广泛应用于图像去噪、信号恢复、协同过滤等领域。\n",
    "\n",
    "---\n",
    "\n",
    "#### 总结\n",
    "\n",
    "| **范数**   | **定义**                                         | **性质**                                         | **常见应用**                                           |\n",
    "|------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------------|\n",
    "| **0范数**  | 非零元素的个数                                   | 不满足常规范数条件，常用于稀疏性                  | 特征选择，稀疏编码                                    |\n",
    "| **1范数**  | 向量元素绝对值之和                               | 对稀疏性友好，可使部分元素为零                    | LASSO回归，稀疏编码，压缩感知                        |\n",
    "| **2范数**  | 向量元素平方和的平方根                           | 对异常值更为鲁棒，平滑                            | Ridge回归，机器学习中的常见损失函数，距离计算        |\n",
    "| **p范数**  | 向量元素绝对值的p次方和的p次方根                 | 对大元素更敏感，p越大对极值越敏感                 | 最优化问题，正则化，调整平滑性                        |\n",
    "| **核范数** | 矩阵奇异值的和                                   | 用于低秩矩阵恢复，矩阵补全                        | 矩阵补全，推荐系统，低秩矩阵恢复                      |\n",
    "\n",
    "这些范数在 **机器学习、深度学习、信号处理** 和 **图像处理** 等多个领域有着广泛的应用，能够帮助我们在优化模型时控制参数的大小或稀疏性，解决实际问题中的多种挑战。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7411],\n",
      "        [0.4294]]) tensor([[0.8854],\n",
      "        [0.5739]])\n",
      "tensor(0.2888)\n",
      "tensor(0.2042)\n",
      "tensor(0.1820)\n",
      "tensor(0.8565)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 1)\n",
    "b = torch.rand(2, 1)\n",
    "\n",
    "print(a, b)\n",
    "print(torch.dist(a, b, p = 1 ))\n",
    "print(torch.dist(a, b, p = 2 ))\n",
    "print(torch.dist(a, b, p = 3 ))\n",
    "\n",
    "\n",
    "print(torch.norm(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 022 to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
