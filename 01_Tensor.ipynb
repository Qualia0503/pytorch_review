{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00 Tensor是什么\n",
    "张量，描述任意物体复杂维度；    \n",
    "标量，零维表格；     \n",
    "向量，一维表格；    \n",
    "矩阵，二维表格；    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习；    \n",
    "$ y = wx + b $      \n",
    "w, b在未知时候，就是变量\n",
    "\n",
    "样本 模型       \n",
    "样本——Tensor；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor核心概念:     \n",
    "类型——创建；            \n",
    "属性——运算；\n",
    "操作——Numpy的互相转换；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的类型    \n",
    "float32, float16, uint8, int8, int6, int32, int64, bool     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img_Tensor的创建](img_Tensor的创建.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.FloatTensor\n",
      "tensor([[-1.1344e-36,  7.3848e-43,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "torch.FloatTensor\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.FloatTensor\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n",
      "torch.FloatTensor\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "\n",
    "a = torch.Tensor(2, 3)\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "a = torch.ones(2, 3)\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "a = torch.eye(2, 3)\n",
    "print(a)\n",
    "print(a.type())\n",
    "\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.Tensor(2, 3)\n",
    "b = torch.zeros_like(b)\n",
    "b = torch.ones_like(b)\n",
    "\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1593, 0.8700],\n",
      "        [0.6065, 0.9481]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机的生成一些满足一定分布的张量；      \n",
    "正态分布，normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1593, 0.8700],\n",
      "        [0.6065, 0.9481]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "torch.normal(mean=0.0, std=torch.rand(5))\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1593, 0.8700],\n",
      "        [0.6065, 0.9481]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "torch.normal(mean=torch.rand(5), std=torch.rand(5))\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机生成均匀分布的数值uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3392,  0.3145],\n",
      "        [-0.3022,  0.4031]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(2, 2).uniform_(-1, 1)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成一些序列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 10, 1)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成等间隔的n个数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  6., 10.])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(2, 10, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 9, 2, 3, 1, 8, 4, 7, 5, 6])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.randperm(10)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比numpy 和tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Tensor的属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个Tensor有torch.dtype, torch.device, torch.layout三种属性           \n",
    "+ torch.device标识了torch.Tensor对象在创建之后所存储在的设备的名称        \n",
    "+ torch.layout表示torch.Tensor内存布局的对象(稠密张量或者稀疏张量，稠密张量一般直接存在内存中的连续区域，稀疏张量只存储其非零坐标)        \n",
    "+ torch.tensor([1,2,3], dtype=torch.float32, device = torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稀疏张量的定义方式          \n",
    "torch.sparse_coo_tensor\n",
    "coo类型表示了非零元素的坐标形式     \n",
    "x = torch.sparse_coo_tensor(i, v, [2, 4])       \n",
    "稀疏与低秩      \n",
    "+ 稀疏描述了一个张量中的非零元素的多少        \n",
    "+ 秩表示了当前张量中的线性可表示关系      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dev = torch.device('cpu')\n",
    "dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2, 2], \n",
    "                 dtype=torch.float32,\n",
    "                 device=dev)\n",
    "print(a)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 3., 0.],\n",
      "        [0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor([[0, 1, 2], [0, 1, 2]])\n",
    "v = torch.tensor([1, 2, 3])\n",
    "a = torch.sparse_coo_tensor(i, v, (4, 4),\n",
    "                                   dtype=torch.float32,\n",
    "                                      device=dev).to_dense()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Tensor的算术运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法运算        \n",
    "c = a + b   \n",
    "c = torch.add(a, b)     \n",
    "a.add(b)        \n",
    "a.add_(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.6249, 0.7071, 0.0352],\n",
      "        [0.7800, 0.2171, 0.4086]])\n",
      "tensor([[1.4099, 1.5856, 0.1547],\n",
      "        [1.2402, 1.1627, 0.6898]])\n",
      "tensor([[1.4099, 1.5856, 0.1547],\n",
      "        [1.2402, 1.1627, 0.6898]])\n",
      "tensor([[1.4099, 1.5856, 0.1547],\n",
      "        [1.2402, 1.1627, 0.6898]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[1.4099, 1.5856, 0.1547],\n",
      "        [1.2402, 1.1627, 0.6898]])\n",
      "tensor([[1.4099, 1.5856, 0.1547],\n",
      "        [1.2402, 1.1627, 0.6898]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(a + b)\n",
    "print(a.add(b))\n",
    "print(torch.add(a, b))\n",
    "print(a)\n",
    "print(a.add_(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减法运算        \n",
    "c = a - b       \n",
    "c = torch.sub(a, b)     \n",
    "a.sub(b)        \n",
    "a.sub_(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[1.4099, 1.5856, 0.1547],\n",
      "        [1.2402, 1.1627, 0.6898]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "print(a.sub(b))\n",
    "print(torch.sub(a, b))\n",
    "print(a)\n",
    "print(a.sub_(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乘法运算        \n",
    "+ 哈达玛积（element wise, 对应元素相乘）！注意与矩阵运算不同！      \n",
    "\n",
    "c = a * b   \n",
    "c = torch.mul(a, b)     \n",
    "a.mul(b)        \n",
    "a.mul_(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4905, 0.6212, 0.0042],\n",
      "        [0.3590, 0.2053, 0.1149]])\n",
      "tensor([[0.4905, 0.6212, 0.0042],\n",
      "        [0.3590, 0.2053, 0.1149]])\n",
      "tensor([[0.4905, 0.6212, 0.0042],\n",
      "        [0.3590, 0.2053, 0.1149]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.4905, 0.6212, 0.0042],\n",
      "        [0.3590, 0.2053, 0.1149]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "print(torch.mul(a, b))\n",
    "print(a.mul(b))\n",
    "print(a)\n",
    "print(a.mul_(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除法运算        \n",
    "c = a/b     \n",
    "c = torch.div(a, b)     \n",
    "a.div(b)        \n",
    "a.div_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n",
      "tensor([[0.4905, 0.6212, 0.0042],\n",
      "        [0.3590, 0.2053, 0.1149]])\n",
      "tensor([[0.7850, 0.8786, 0.1194],\n",
      "        [0.4603, 0.9456, 0.2812]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "print(torch.div(a, b))\n",
    "print(a.div(b))\n",
    "print(a)\n",
    "print(a.div_(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵运算        \n",
    "+ 二维矩阵乘法运算操作包括torch.mm()、torch.matmul()、@     \n",
    "\n",
    "    a = torch.ones(2, 1)        \n",
    "    b = torch.ones(1, 2)        \n",
    "    print(torch.mm(a, b))       \n",
    "    print(torch.matmul(a, b))       \n",
    "    print(a @ b)        \n",
    "    print(a.matmul(b))      \n",
    "    print(a.mm(b))      \n",
    "\n",
    "\n",
    "    a x b的矩阵必须要保证a是m乘n，b是n乘P   \n",
    "\n",
    "+ 对于高维的Tensor(dim > 2)，定义其矩阵乘法仅在最后的两个维度上，       \n",
    "要求前面的维度必须保持一致，就像矩阵的索引一样并且运算操只有torch.matmul()\n",
    "\n",
    "    a = torch.ones(1, 2, 3, 4)      \n",
    "    b = torch.ones(1, 2, 4, 4)      \n",
    "    print(a.matmul(b))      \n",
    "    print(torch.matmul(a, b))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[1., 1.]])\n",
      "_______________\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 1)        \n",
    "b = torch.ones(1, 2)        \n",
    "print(a)\n",
    "print(b)\n",
    "print(\"_______________\")\n",
    "print(a @ b)\n",
    "print(a.matmul(b))\n",
    "print(torch.matmul(a, b))\n",
    "print(torch.mm(a, b))\n",
    "print(a.mm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]],\n",
      "\n",
      "         [[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]]]])\n",
      "**************************\n",
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1, 2, 3, 4)\n",
    "b = torch.ones(1, 2, 4, 3)\n",
    "\n",
    "\n",
    "print(a.matmul(b))\n",
    "print(\"**************************\")\n",
    "print(a.matmul(b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幂运算      \n",
    "\n",
    "print(torch.pow(a, 2))      \n",
    "print(a.pow(2))     \n",
    "print(a**2)     \n",
    "print(a.pow_(2))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 8])\n",
      "tensor([1, 8])\n",
      "tensor([1, 8])\n",
      "tensor([1, 8])\n",
      "tensor([1, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "print(torch.pow(a, 3))\n",
    "print(a.pow(3))\n",
    "print(a**3)\n",
    "print(a.pow_(3))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e的n次方\n",
    "\n",
    "print(torch.exp(a))     \n",
    "b = a.exp_()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183, 7.3891])\n",
      "tensor([2.7183, 7.3891])\n",
      "tensor([  15.1543, 1618.1782])\n",
      "tensor([  15.1543, 1618.1782])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0])\n",
    "print(torch.exp(a))\n",
    "print(torch.exp_(a))\n",
    "print(a.exp())\n",
    "print(a.exp_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开方运算        \n",
    "\n",
    "print(a.sqrt())     \n",
    "print(a.sqrt_())        \n",
    "print(a)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  2.])\n",
      "tensor([3.1623, 1.4142])\n",
      "tensor([3.1623, 1.4142])\n",
      "tensor([3.1623, 1.4142])\n",
      "tensor([1.7783, 1.1892])\n",
      "tensor([1.7783, 1.1892])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10.0, 2.0])\n",
    "print(a)\n",
    "print(torch.sqrt(a))\n",
    "print(torch.sqrt_(a))\n",
    "print(a)\n",
    "print(a.sqrt())\n",
    "print(a.sqrt_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数运算        \n",
    "\n",
    "print(torch.log2(a))        \n",
    "print(torch.log10(a))       \n",
    "print(torch.log(a))     \n",
    "print(torch.log_(a))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3026, 0.6931])\n",
      "tensor([2.3026, 0.6931])\n",
      "tensor([ 0.8340, -0.3665])\n",
      "tensor([ 0.8340, -0.3665])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10.0, 2.0])\n",
    "print(torch.log(a))\n",
    "print(torch.log_(a))\n",
    "print(a.log())\n",
    "print(a.log_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Pytorch中的in-place操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ “就地”操作，即不允许使用临时变量。          \n",
    "+ 也称为原位操作。        \n",
    "+ x = x + y       \n",
    "+ add_，sub_, mul_等等        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 广播机制\n",
    "+ 广播机制：张量参数可以自动扩展为相同大小（类似于Numpy)\n",
    "+ 广播机制需要满足两个条件：\n",
    "    - 每个张量至少有一个维度\n",
    "    - 满足右对齐\n",
    "    - torch.rand(2, 1, 1)+torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7309, 0.4898, 0.6813],\n",
      "        [0.7110, 0.8748, 0.5183]])\n",
      "tensor([0.6480, 0.0669, 0.8442])\n",
      "tensor([[1.3789, 0.5568, 1.5255],\n",
      "        [1.3590, 0.9417, 1.3624]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05 取整和取余运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5960, 8.5302],\n",
      "        [4.4176, 4.8341]])\n",
      "tensor([[5., 8.],\n",
      "        [4., 4.]])\n",
      "tensor([[6., 9.],\n",
      "        [5., 5.]])\n",
      "tensor([[6., 9.],\n",
      "        [4., 5.]])\n",
      "tensor([[5., 8.],\n",
      "        [4., 4.]])\n",
      "tensor([[0.5960, 0.5302],\n",
      "        [0.4176, 0.8341]])\n",
      "tensor([[1.5960, 0.5302],\n",
      "        [0.4176, 0.8341]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "a = a * 10\n",
    "print(a)\n",
    "\n",
    "print(torch.floor(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.round(a))\n",
    "print(torch.trunc(a))\n",
    "print(torch.frac(a))\n",
    "print(a % 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 比较运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.eq(input, other, out=None) # 按成员进行等式操作，相同返回True     \n",
    "torch.equal(tensor1, tensor2) # 判断两个张量是否有相同的size 和 elements，则为True      \n",
    "torch.ge(input, other, out=None) # 按成员进行大于等于操作，大于等于返回True     \n",
    "torch.gt(input, other, out=None) # 按成员进行大于操作，大于返回True     \n",
    "torch.le(input, other, out=None) # 按成员进行小于等于操作，小于等于返回True     \n",
    "torch.lt(input, other, out=None) # 按成员进行小于操作，小于返回True     \n",
    "torch.ne(input, other, out=None) # 按成员进行不等式操作，不相同返回True     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4737, 0.9322, 0.8486],\n",
      "        [0.5744, 0.6898, 0.4550]])\n",
      "tensor([[0.7290, 0.8597, 0.0208],\n",
      "        [0.9196, 0.5409, 0.3582]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "False\n",
      "tensor([[False,  True,  True],\n",
      "        [False,  True,  True]])\n",
      "tensor([[False,  True,  True],\n",
      "        [False,  True,  True]])\n",
      "tensor([[ True, False, False],\n",
      "        [ True, False, False]])\n",
      "tensor([[ True, False, False],\n",
      "        [ True, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(torch.eq(a, b))\n",
    "print(torch.equal(a, b))\n",
    "print(torch.ge(a, b))\n",
    "print(torch.gt(a, b))\n",
    "print(torch.le(a, b))\n",
    "print(torch.lt(a, b))\n",
    "print(torch.ne(a, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 取前k大/前k小/第k小的数值及其索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ torch.sort(input, dim=None, descending = False, out=None) #对目标张量input沿着指定维按升序排序      \n",
    "+ torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) #沿着指定维度返回最大k个数值及其索引值      \n",
    "    这个一般用在深度学习中对loss函数进行定义时候            \n",
    "+ torch.kthvalue(input, k, dim=None, keepdim=False, out=None) #沿着指定维度返回第k个最小值及其索引值      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([1, 3, 4, 4, 5]),\n",
      "indices=tensor([0, 3, 1, 2, 4]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 4, 4, 3, 5])\n",
    "print(torch.sort(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.sort(\n",
      "values=tensor([[5, 4, 4, 3, 1],\n",
      "        [5, 3, 3, 2, 1]]),\n",
      "indices=tensor([[4, 1, 2, 3, 0],\n",
      "        [4, 1, 3, 0, 2]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 4, 4, 3, 5],\n",
    "                  [2, 3, 1, 3, 5]])\n",
    "print(a.shape)\n",
    "print(torch.sort(a, dim = 1, descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[2, 4, 5, 1, 5],\n",
      "        [2, 3, 3, 1, 4]]),\n",
      "indices=tensor([[0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 1]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[2, 4, 5, 1, 5]]),\n",
      "indices=tensor([[0, 0, 1, 0, 0]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[5],\n",
      "        [5]]),\n",
      "indices=tensor([[4],\n",
      "        [2]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[5, 4],\n",
      "        [5, 4]]),\n",
      "indices=tensor([[4, 1],\n",
      "        [2, 4]]))\n"
     ]
    }
   ],
   "source": [
    "##topk\n",
    "a = torch.tensor([[2, 4, 3, 1,5],\n",
    "                  [2, 3, 5, 1, 4]])\n",
    "print(a.shape)\n",
    "\n",
    "print(torch.topk(a, k = 2, dim=0))\n",
    "print(torch.topk(a, k = 1, dim=0))\n",
    "print(torch.topk(a, k = 1, dim=1))\n",
    "print(torch.topk(a, k = 2, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.kthvalue(\n",
      "values=tensor([2, 4, 5, 1, 5]),\n",
      "indices=tensor([1, 0, 1, 1, 0]))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([2, 2]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.kthvalue(a, k = 2, dim=0))\n",
    "print(torch.kthvalue(a, k = 2, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 判断是否为finite/inf/nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ torch.isfinite(tensor)/torch.isinf(tensor)/torch.isnan(tensor)      \n",
    "+ 返回一个标记元素是否为finite/inf/nan的mask张量      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9986, 0.8089, 0.1919],\n",
      "        [0.0518, 0.0589, 0.2405]])\n",
      "tensor([[inf, inf, inf],\n",
      "        [inf, inf, inf]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(a/0)\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isfinite(a/0))\n",
    "print(torch.isinf(a/0))\n",
    "print(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True])\n",
      "tensor([False, False, False])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([1, 2, np.nan])\n",
    "print(torch.isnan(a))\n",
    "print(torch.isinf(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 三角函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.abs(input, out=None)      \n",
    "torch.acos(input, out=None)     \n",
    "torch.asin(input, out=None)     \n",
    "torch.atan(input, out=None)     \n",
    "torch.atan2(input1, input2, out=None)       \n",
    "torch.cos(input, out=None)      \n",
    "torch.cosh(input, out=None)     \n",
    "torch.sin(input, out=None)      \n",
    "torch.sinh(input, out=None)     \n",
    "torch.tan(input, out=None)      \n",
    "torch.tanh(input, out=None)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3427, 0.0664, 0.6329],\n",
      "        [0.8589, 0.6158, 0.2462]])\n",
      "tensor([[0.9419, 0.9978, 0.8063],\n",
      "        [0.6533, 0.8163, 0.9698]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "b = torch.cos(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 其它的数学函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.asb()     \n",
    "torch.erf()     \n",
    "torch.erfinv()      \n",
    "torch.sigmoid()     \n",
    "torch.neg()     \n",
    "torch.reciprocal()      \n",
    "torch.rsqrt()       \n",
    "torch.sign()        \n",
    "torch.lerp()        \n",
    "torch.addcdiv()     \n",
    "torch.addcmul()     \n",
    "torch.cumprod()     \n",
    "torch.cumsum()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 统计学有关的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.mean()   #平均值      \n",
    "torch.sum()    #求和            \n",
    "torch.prod()   #求积            \n",
    "torch.max()    #最大值      \n",
    "torch.min()    #最小值      \n",
    "torch.argmax() #最大值索引      \n",
    "torch.argmin() #最小值索引      \n",
    "\n",
    "\n",
    "torch.median() #中位数      \n",
    "torch.std()    #标准差      \n",
    "torch.var()    #方差        \n",
    "torch.mode()   #众数        \n",
    "torch.histc()  #直方图统计      \n",
    "torch.bincount() #计数      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7996, 0.4317],\n",
      "        [0.7420, 0.8674]])\n",
      "tensor(0.7102)\n",
      "tensor(2.8408)\n",
      "tensor(0.2222)\n",
      "tensor(0.1926)\n",
      "tensor(0.0371)\n",
      "tensor(0.7420)\n",
      "torch.return_types.mode(\n",
      "values=tensor([0.4317, 0.7420]),\n",
      "indices=tensor([1, 0]))\n",
      "tensor(1.4590)\n",
      "tensor(0.)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "\n",
    "print(a)\n",
    "print(torch.mean(a))\n",
    "print(torch.sum(a))\n",
    "print(torch.prod(a))\n",
    "print(torch.std(a))\n",
    "print(torch.var(a))\n",
    "print(torch.median(a))\n",
    "print(torch.mode(a))\n",
    "print(torch.norm(a))\n",
    "print(torch.dist(a, a))\n",
    "print(torch.histc(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.1951, 1.0260],\n",
      "        [0.2502, 7.0497]])\n",
      "tensor([2., 0., 2., 0.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "\n",
    "print(torch.histc(a, bins=4, min=0, max=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 分布函数torch.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如机器学习中的训练集和测试集的分布是否一致      \n",
    "\n",
    "- distributions包含可参数化的概率分布和采样函数             \n",
    "    - 得分函数           \n",
    "         - 强化学习中策略梯度方法的基础               \n",
    "    - pathwise derivative估计器       \n",
    "        - 变分自动编码器中的重新参数化技巧        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 随机抽样函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义随机种子          \n",
    "    - torch.manual_seed(seed)       \n",
    "- 定义随机满足的分布        \n",
    "    - torch.normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9720, 1.1360]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mean = torch.rand(1, 2)\n",
    "std  = torch.rand(1, 2)\n",
    "\n",
    "print(torch.normal(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 范数\n",
    "    - 在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即1.非负性；2齐次性；3.三角不等式\n",
    "    - 常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小\n",
    "\n",
    "- 0范数/1范数/2范数/p范数/核范数\n",
    "    - torch.dist(input, other, p=2)计算p范数\n",
    "    - torch.norm() 计算2范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补：向量与矩阵的范数\n",
    "\n",
    "范数（Norm）是一个数学概念，表示向量或矩阵的“大小”或“长度”，        \n",
    "它是衡量向量或矩阵在空间中某种距离的方式。在数学、物理学和计算机科学中，        \n",
    "范数常用于优化、距离计算、机器学习和信号处理等领域。\n",
    "\n",
    "以下是几种常见的范数：0范数、1范数、2范数、p范数、核范数，      \n",
    "它们通常用于向量和矩阵的计算中，不同的范数具有不同的特性和应用。        \n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 0 范数（零范数，L0 Norm）       \n",
    "**定义**：零范数并不是真正的范数，但它常常被用来计算一个向量或矩阵中非零元素的个数。        \n",
    "用公式表示为：      \n",
    "$ \\|x\\|_0 = \\text{非零元素的个数} $\n",
    "\n",
    "**性质**：\n",
    "- 零范数不是严格意义上的范数，因为它不满足**齐次性**（例如，$ \\| \\alpha x \\|_0 \\neq \\alpha \\| x \\|_0 $，对于标量 $ \\alpha $）。\n",
    "- 但是，零范数通常用于**稀疏性**相关的任务中，比如 **LASSO回归**、**特征选择**等。\n",
    "\n",
    "**应用**：\n",
    "- **稀疏表示**：通过零范数可以衡量一个向量或矩阵有多少个非零元素。      \n",
    "在机器学习中，通常希望得到一个具有较少非零元素的模型，这样的模型更加稀疏、简单、易于解释。      \n",
    "- **特征选择**：通过零范数来选出最重要的特征，抑制不重要的特征。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 1 范数（L1 Norm）\n",
    "**定义**：1范数是向量中所有元素绝对值的和，公式如下：\n",
    "$ \\|x\\|_1 = \\sum_i |x_i| $\n",
    "\n",
    "**性质**：\n",
    "- L1 范数会使得一些元素变为 0，因此常用于实现**稀疏化**。\n",
    "- 它对于噪声较敏感，但能够很好地处理**稀疏问题**。\n",
    "\n",
    "**应用**：\n",
    "- **LASSO回归**：L1范数用于正则化（惩罚模型的复杂度），     \n",
    "通过约束模型参数的 L1 范数来使得一些系数变为 0，从而选择重要特征。      \n",
    "- **稀疏编码和压缩感知**：L1范数用于找到稀疏表示，减少数据的冗余。      \n",
    "  \n",
    "---\n",
    "\n",
    "#### 3. 2 范数（L2 Norm）\n",
    "**定义**：2范数，也称为 **欧几里得范数**，是向量各元素的平方和的平方根，公式如下：\n",
    "$ \\|x\\|_2 = \\sqrt{\\sum_i x_i^2} $\n",
    "\n",
    "**性质**：\n",
    "- L2 范数与欧几里得距离相关，常用于计算两点之间的距离。\n",
    "- L2 范数的优点是其具有**平滑性**，不会导致像 L1 范数那样产生稀疏解。\n",
    "- L2 范数对于异常值更为**鲁棒**，相比 L1 范数，它的计算结果更为平滑。\n",
    "\n",
    "**应用**：\n",
    "- **Ridge回归**：L2范数用于正则化，通过约束参数的L2范数来控制模型的复杂度，避免过拟合。\n",
    "- **梯度下降算法**：L2范数常用于优化问题中，特别是当我们需要最小化误差平方和时。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. p 范数（Lp Norm）\n",
    "**定义**：p范数是对任意 $p \\geq 1$ 的扩展，公式如下：\n",
    "$ \\|x\\|_p = \\left( \\sum_i |x_i|^p \\right)^{1/p} $       \n",
    "当 $p = 1$ 时，得到的是 **L1范数**，当 $p = 2$ 时，得到的是 **L2范数**，        \n",
    "当 $p \\to \\infty$ 时，得到的是 **最大值范数**（即最大元素的绝对值）。       \n",
    "\n",
    "**性质**：\n",
    "- 当 $p = 1$，是 L1 范数；\n",
    "- 当 $p = 2$，是 L2 范数；\n",
    "- 随着 $p$ 增加，p 范数变得对大元素更敏感，尤其是 $p$ 较大时，p 范数更加注重向量中最大元素的影响。\n",
    "\n",
    "**应用**：\n",
    "- **正则化**：p范数的正则化（例如，L1正则化和L2正则化）用于约束模型的参数，防止过拟合。\n",
    "- **最优解的选择**：在机器学习中，根据问题需要选择不同的 p 范数进行优化，p 范数适用于不同的平滑性需求。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 核范数（Nuclear Norm）\n",
    "**定义**：核范数是一个矩阵的所有奇异值的和，也可以理解为矩阵的 **L1 范数** 对奇异值的扩展。     \n",
    "对于矩阵 $X$，其核范数定义为：      \n",
    "$ \\|X\\|_* = \\sum_i \\sigma_i $       \n",
    "其中 $ \\sigma_i $ 是矩阵 $X$ 的奇异值。     \n",
    "\n",
    "**性质**：\n",
    "- 核范数广泛用于矩阵补全问题中，能够得到**低秩矩阵**。\n",
    "- 核范数与 **L1 范数** 类似，但它作用于矩阵的奇异值，而不是矩阵元素本身。\n",
    "  \n",
    "**应用**：\n",
    "- **矩阵补全**：在缺失数据的情况下，使用核范数来逼近低秩矩阵，常用于推荐系统（如Netflix的电影推荐）。\n",
    "- **低秩矩阵恢复**：核范数正则化能够找到矩阵的低秩近似，广泛应用于图像去噪、信号恢复、协同过滤等领域。\n",
    "\n",
    "---\n",
    "\n",
    "#### 总结\n",
    "\n",
    "| **范数**   | **定义**                                         | **性质**                                         | **常见应用**                                           |\n",
    "|------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------------|\n",
    "| **0范数**  | 非零元素的个数                                   | 不满足常规范数条件，常用于稀疏性                  | 特征选择，稀疏编码                                    |\n",
    "| **1范数**  | 向量元素绝对值之和                               | 对稀疏性友好，可使部分元素为零                    | LASSO回归，稀疏编码，压缩感知                        |\n",
    "| **2范数**  | 向量元素平方和的平方根                           | 对异常值更为鲁棒，平滑                            | Ridge回归，机器学习中的常见损失函数，距离计算        |\n",
    "| **p范数**  | 向量元素绝对值的p次方和的p次方根                 | 对大元素更敏感，p越大对极值越敏感                 | 最优化问题，正则化，调整平滑性                        |\n",
    "| **核范数** | 矩阵奇异值的和                                   | 用于低秩矩阵恢复，矩阵补全                        | 矩阵补全，推荐系统，低秩矩阵恢复                      |\n",
    "\n",
    "这些范数在 **机器学习、深度学习、信号处理** 和 **图像处理** 等多个领域有着广泛的应用，能够帮助我们在优化模型时控制参数的大小或稀疏性，解决实际问题中的多种挑战。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9408],\n",
      "        [0.1332]]) tensor([[0.9346],\n",
      "        [0.5936]])\n",
      "tensor(0.4666)\n",
      "tensor(0.4604)\n",
      "tensor(0.4604)\n",
      "tensor(0.9502)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 1)\n",
    "b = torch.rand(2, 1)\n",
    "\n",
    "print(a, b)\n",
    "print(torch.dist(a, b, p = 1 ))\n",
    "print(torch.dist(a, b, p = 2 ))\n",
    "print(torch.dist(a, b, p = 3 ))\n",
    "\n",
    "\n",
    "print(torch.norm(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 矩阵分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 常见的矩阵分解\n",
    "    - LU分解：将矩阵A分解为L（下三角）矩阵和U（上三角）矩阵的乘积       \n",
    "    - QR分解：将原矩阵分解为一个正交矩阵Q和一个上三角矩阵R的乘积        \n",
    "    - EVD分解：特征值分解  PCA      \n",
    "    - SVD分解：奇异值分解  LDA      \n",
    "\n",
    "- 特征值分解\n",
    "    - 将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法\n",
    "    - 特征值 VS 特征向量     \n",
    "\n",
    "            $ A \\mathbf{v} = \\lambda \\mathbf{v} $       \n",
    "            $ A = Q \\Sigma Q^{-1} $\n",
    "\n",
    "- PCA与特征值分解\n",
    "    - PCA：将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征\n",
    "    - PCA算法的优化目标是“\n",
    "        - 降维后同一维度的方差最大\n",
    "        - 不同维度之间的相关性为0\n",
    "        - 协方差矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14 矩阵分解-SVD分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 奇异值分解，即SVD分解\n",
    "\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "\n",
    "- LDA与奇异值分解       \n",
    "- EVD分解 VS SVD分解        \n",
    "    - 矩阵方阵且满秩（可对角化）        \n",
    "    - 矩阵分解不等于特征降维度      \n",
    "    - 协防长矩阵描述方差喝相关性        \n",
    "\n",
    "- Pytorch中的奇异值分解\n",
    "    - torch.svd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 Tensor的裁剪运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对Tensor中的元素范围进行过滤      \n",
    "- 常用于梯度裁剪（gradient clipping），即在发生梯度离散或者梯度爆炸时对梯度的处理       \n",
    "- a.clamp(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3723, 2.4819],\n",
      "        [7.2736, 0.4597]])\n",
      "tensor([[2.3723, 2.4819],\n",
      "        [5.0000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "\n",
    "a = a.clamp(2, 5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 张量的索引与数据筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.where(condition, x, y): 按照条件从x和y中选满足条件的元素组成新的tensor      \n",
    "- torch.gather(input, dim, index, out=None)：在指定维度上按照索引赋值输出tensor     \n",
    "- torch.index_select(input, dim, index, out=None)：按照指定索引输出tensor       \n",
    "- torch.masked_select(input, mask, out=None)：按照mask输出tensor，输出为向量        \n",
    "- torch.take(input, indices)：将输入看成1D-tensor，按照索引得到输出tensor       \n",
    "- torch.nonzero(input， out=None)：输出非0元素的坐标        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6033, 0.5812, 0.2200, 0.4279],\n",
      "        [0.5222, 0.2453, 0.4955, 0.6322],\n",
      "        [0.1450, 0.9558, 0.6763, 0.0885],\n",
      "        [0.7301, 0.8784, 0.3074, 0.4570]])\n",
      "tensor([[0.2586, 0.1397, 0.0736, 0.6076],\n",
      "        [0.0181, 0.1982, 0.1275, 0.4236],\n",
      "        [0.1943, 0.3333, 0.6483, 0.5913],\n",
      "        [0.4964, 0.4235, 0.7366, 0.0802]])\n",
      "tensor([[0.6033, 0.5812, 0.0736, 0.6076],\n",
      "        [0.5222, 0.1982, 0.1275, 0.6322],\n",
      "        [0.1943, 0.9558, 0.6763, 0.5913],\n",
      "        [0.7301, 0.8784, 0.7366, 0.0802]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Where\n",
    "a = torch.rand(4, 4)\n",
    "b = torch.rand(4, 4)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.where(a > 0.5, a, b)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.2330, 0.3823, 0.5255],\n",
      "        [0.8645, 0.0057, 0.6850, 0.6673],\n",
      "        [0.0357, 0.9737, 0.2243, 0.4482],\n",
      "        [0.6749, 0.7086, 0.7671, 0.2800]])\n",
      "tensor([[0.3414, 0.2330, 0.3823, 0.5255],\n",
      "        [0.6749, 0.7086, 0.7671, 0.2800],\n",
      "        [0.0357, 0.9737, 0.2243, 0.4482]])\n"
     ]
    }
   ],
   "source": [
    "# torch.index_select\n",
    "\n",
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "b = torch.index_select(a, dim=0, index = torch.tensor([0, 3, 2]))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor([[ 1.,  6.,  7.,  8.],\n",
      "        [ 1.,  6., 11., 12.],\n",
      "        [ 1.,  6., 15., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.gather\n",
    "\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "\n",
    "print(a)\n",
    "\n",
    "out = torch.gather(a, dim=0, index=torch.tensor([[0, 1, 1, 1],\n",
    "                                           [0, 1, 2, 2],\n",
    "                                           [0, 1, 3, 3]]))\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [ True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True]])\n",
      "tensor([ 9., 10., 11., 12., 13., 14., 15., 16.])\n"
     ]
    }
   ],
   "source": [
    "#  torch.masked_select\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "print(a)\n",
    "mask = torch.gt(a, 8)\n",
    "print(mask)\n",
    "out = torch.masked_select(a, mask)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor([ 1.,  6., 11., 16.])\n"
     ]
    }
   ],
   "source": [
    "# torch.take\n",
    "\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "print(a)\n",
    "\n",
    "out = torch.take(a, torch.tensor([0, 5, 10, 15]))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 0],\n",
      "        [2, 3, 0, 1]])\n",
      "tensor([[0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 3]])\n"
     ]
    }
   ],
   "source": [
    "# torch.nonzero\n",
    "\n",
    "a = torch.tensor([[0, 1, 2, 0],\n",
    "                  [2, 3, 0, 1]])\n",
    "print(a)\n",
    "out = torch.nonzero(a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 张量的组合与拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cat(seq, dim=0, out=None)：按照已经存在的维度进行拼接       \n",
    "- torch.stack(seq, dim=0, out=None)：按照新的维度进行拼接       \n",
    "- torch.gather(input,dim,index,out=None)：在指定维度上按照索引赋值输出tensor        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros((2, 4))\n",
    "b = torch.ones((2, 4))\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.cat((a, b), dim=0)\n",
    "print(out)\n",
    "\n",
    "out = torch.cat((a, b), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [10., 11., 12.]]])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.],\n",
      "         [10., 11., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "# stack\n",
    "\n",
    "a  = torch.linspace(1, 6, 6).view(2, 3)\n",
    "b  = torch.linspace(7, 12, 6).view(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.stack((a, b), dim=0)\n",
    "print(out)\n",
    "\n",
    "out = torch.stack((a, b), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18 张量的切片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.chunk(tensor, chunks, dim=0) 按照某个维度平均分块（最后一个可能小于平均值）     \n",
    "- torch.split(tensor, split_size_or_sections, dim=0) 按照某个维度依照第二个参数给出的list或者int进行分割tensor      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6562, 0.3315, 0.3690, 0.6879],\n",
      "        [0.1346, 0.2862, 0.9711, 0.8135],\n",
      "        [0.3375, 0.7985, 0.5453, 0.8104]])\n",
      "tensor([[0.6562, 0.3315],\n",
      "        [0.1346, 0.2862],\n",
      "        [0.3375, 0.7985]]) torch.Size([3, 2])\n",
      "tensor([[0.3690, 0.6879],\n",
      "        [0.9711, 0.8135],\n",
      "        [0.5453, 0.8104]]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((3, 4))\n",
    "\n",
    "print(a)\n",
    "out = torch.chunk(a, 2, dim=1)\n",
    "print(out[0], out[0].shape)\n",
    "print(out[1], out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.6562, 0.3315, 0.3690, 0.6879],\n",
      "        [0.1346, 0.2862, 0.9711, 0.8135],\n",
      "        [0.3375, 0.7985, 0.5453, 0.8104]]),)\n"
     ]
    }
   ],
   "source": [
    "out = torch.split(a, 3, dim=0)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19 张量变形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.reshape(input, shape)       \n",
    "- torch.t(input)： 只针对2D tensor转置      \n",
    "- torch.transpose(input, dim0, dim1)：交换两个维度      \n",
    "- torch.squeeze(input, dim = None, out = None)：去除那些维度大小为1的维度       \n",
    "- torch.unbind(tensor, dim=0): 去除某个维度     \n",
    "- torch.unsqueeze(input, dim, out=None)：在指定位置添加维度     \n",
    "- torch.flip(input, dims)：按照给定维度翻转张量     \n",
    "- torch.rot90(input, k, dims)：按照指定维度和旋转次数进行张量旋转       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1457, 0.8102, 0.7798],\n",
      "        [0.6540, 0.7154, 0.2918]])\n",
      "tensor([[0.1457, 0.8102],\n",
      "        [0.7798, 0.6540],\n",
      "        [0.7154, 0.2918]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "out = torch.reshape(a, (3, 2))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1457, 0.7798, 0.7154],\n",
       "        [0.8102, 0.6540, 0.2918]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1457, 0.7798, 0.7154],\n",
       "        [0.8102, 0.6540, 0.2918]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(out, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7486, 0.1014, 0.3502]],\n",
      "\n",
      "        [[0.4669, 0.5676, 0.7732]]])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 2, 3)\n",
    "print(torch.transpose(a, 0, 1))\n",
    "print(torch.transpose(a, 0, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 张量的填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义Tensor, 并填充指定的数值      \n",
    "    - torch.full((2, 3), 3.14)\n",
    "        - tensor([[3.14, 3.14, 3.14], [3.14, 3.14, 3.14]])      \n",
    "\n",
    "- 频谱操作      \n",
    "    - torch.fft等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21 Pytorch的其它技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型的保存/加载\n",
    "    - torch.saves(state, dir)     保存/序列化   \n",
    "    - torch.load(dir)   加载模型        \n",
    "\n",
    "- 并行化        \n",
    "    - torch.get_num_threads():      \n",
    "        - 获得用于并行化CPU操作的OpenMP线程数   \n",
    "    - torch.set_num_threads(int)：      \n",
    "        - 设定用于并行化CPU操作的OpenMP线程数\n",
    "\n",
    "- 分布式\n",
    "    - Python默认只使用一个GPU，在多个GPU的情况下就需要使用Pytorch提供的DataParallel     \n",
    "    - 单机多卡      \n",
    "    - 多机多卡      \n",
    "\n",
    "- Tensor on GPU\n",
    "- 一般进行数据处理时候放CPU上，进行模型操作时候放GPU上；        \n",
    "- 用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动      \n",
    "- 如下示例代码：        \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")       #GPU        \n",
    "        y = torch.ones_like(x, device=device) # 直接创建一个在GPU上的Tensor       \n",
    "        x = x.to(device)          \n",
    "        z = x + y       \n",
    "        print(z)        \n",
    "        print(z.to(\"cpu\", torch.double))    # to()还可以同时更改数据类型        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensor的相关配置\n",
    "\n",
    "    - torch.is tensor()#如果是pytorch的tensor类型返回true   \n",
    "    - torch.is storage()#如果是pytorch的storage类型返回ture     \n",
    "    - torch.set flush denormal(mode)#防止一些不正常的元素产生       \n",
    "    - torch.set default dtype(d)#对torch.tensor()设置默认的浮点类型     \n",
    "    - torch.set printoptions(precision=None, threshold=None,edgeitems=None,     \n",
    "        linewidth=None, profile=None)# 设置printing的打印参数       \n",
    "    - 等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensor与Numpy的互相转换\n",
    "\n",
    "    - torch.from_numpy(ndarry)      \n",
    "    - a.numpy()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
