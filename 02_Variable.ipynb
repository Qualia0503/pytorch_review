{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 什么是导数        \n",
    "    - 导数（一元函数）是变化率、是切线的斜率、是瞬时速度；\n",
    "    - 可导与不可导      \n",
    "\n",
    "![导数.png](导数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 什么是方向导数        \n",
    "    - 函数在A点无数个切线的斜率的定义。每一个切线都代表一个变化的方向。     \n",
    "![无数个切线.png](无数个切线.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 什么是偏导数      \n",
    "    - 多元函数降维时候的变化，比如：二元函数固定y，只让x单独变化，从而看成是关于x的一元函数的变化来研究     \n",
    "    ![多元函数.png](多元函数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 什么是梯度\n",
    "    - 函数在A点无数个变化方向中变化最快的那个方向。     \n",
    "    - 梯度下降，就是沿着方向变化最快的方向去寻找最优解。        \n",
    "![](梯度.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度与机器学习中的最优解      \n",
    "- 有监督学习，无监督学习，半监督学习    \n",
    "- 样本x，标签y\n",
    "\n",
    "- 机器学习就是找到最小的loss\n",
    "- loss, argmin loss，loss=| f(w,x)-y |^2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable is Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目前Variable 已经与Tensor合并。       \n",
    "- 每个tensor通过requires_grad来设置是否计算梯度。       \n",
    "    - 用来冻结某些层的参数      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何计算梯度\n",
    "- 链式法则：两个函数组合起来的符合函数，导数等于里面函数带入外函数值的导乘以里面函数之导数。        \n",
    "- 举个栗子： y = 2x, z=y*y, 求dz/dx。        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于Autograd的几个概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 叶子张量（leaf)       \n",
    "    ![](叶子张量.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grad VS grad_fn       \n",
    "    - grad: 该Tensor的梯度值，每次在计算backward时都需要将前一时刻的梯度归零，否则梯度值会一直累加；    \n",
    "    - grad_fn：叶子节点通常为None, 只有结果节点的grad_fn才有效，用于指示梯度函数是哪种类型。        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- backward函数      \n",
    "    - torch.autograd.backward(tensor, grad_tensors = None, retain_graph = None, create_graph=False)\n",
    "        - tensor: 用于计算梯度的tensor, torch.autograd.backward(z) == z.backward()\n",
    "        - grad_tensor：在计算矩阵的梯度时会用到。他其实也是一个tensor，shape一般需要和前面的tensor保持一致。    \n",
    "        - retain_graph：通常在调用一次backward后，pytorch会自动把计算图销毁，所以要想对某个变量重复调用backward，则需要将该参数设置为True       \n",
    "        - create_graph: 如果为RTrue, 那么就创建一个专门的graph of the derivaive，这可以方便计算高阶微分。       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.autograd.grad()函数     \n",
    "    - def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n",
    "        - 计算和返回output关于inputs的梯度的和。\n",
    "        - outputs：函数的因变量，即需要求导的那个函数。     \n",
    "        - inputs：函数额自变量，\n",
    "        - grad_outputs：同backward      \n",
    "        - only_inputs：只计算input的梯度        \n",
    "        - allow_unused(bool，可选)：如果为False，当计算输出出错时（因此他们的梯度永远是0）指明不使用的inputs。      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.autograd包中的其他函数      \n",
    "    - torch.autograd.enable_grad：启动梯度计算的上下文管理器        \n",
    "    - torch.autograd.no_grad：禁止梯度计算的上下文管理器        \n",
    "    - torch.autograd.set_grad_enabled(mode): 设置是否进行梯度计算的上下文管理器。       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.autograd.Function       \n",
    "    - 每一个原始的自动求导运算实际上是两个在Tensor上的运行的函数        \n",
    "        - forward函数计算从输入Tensor获得的输出Tensors      \n",
    "        - backward函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。        \n",
    "        - 最后，利用apply方法执行相应的运算     \n",
    "            - 定义在Function类的父类_FunctionBase中定义的一个方法       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class line(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b):\n",
    "        ctx.save_for_backward(w, x, b)\n",
    "        return w*x+b\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        w, x, b = ctx.saved_tensors\n",
    "\n",
    "        grad_w = grad_out*x\n",
    "        grad_x = grad_out*w\n",
    "        grad_b = grad_out\n",
    "\n",
    "        return grad_w, grad_x, grad_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(2, 2, requires_grad=True)\n",
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "b = torch.rand(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2499, 0.3156],\n",
      "        [0.4506, 0.3886]], requires_grad=True) tensor([[0.1759, 0.3622],\n",
      "        [0.5290, 0.3648]], requires_grad=True) tensor([[0.3168, 0.9686],\n",
      "        [0.2804, 0.8752]], requires_grad=True)\n",
      "tensor([[0.1759, 0.3622],\n",
      "        [0.5290, 0.3648]]) tensor([[0.2499, 0.3156],\n",
      "        [0.4506, 0.3886]]) tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "out = line.apply(w, x, b)\n",
    "out.backward(torch.ones(2, 2))\n",
    "\n",
    "print(w, x, b)\n",
    "print(w.grad, x.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
